[{"content":"Big Idea From the start of my career, I have been fascinated by Kubernetes. I love distributed systems and the rich history of how we have arrived where we are today with distributed computing. We live in the ongoing evolution of our communal vision for cloud computing. In the early years, the vision was to present a homogenous Unix-like interface for managing an underlying collection of servers such as BOINC. Now, we live in the world of many small virtualized unix environments distributed across servers and sharing compute.\nOne of the main drivers of our current state has been the advent of containerization and container orchestration. The goal of this blog is to go over the considerations and design of my bare-metal Raspberry Pi Kubernetes cluster. This project was my adventure in going beyond being a user of Kubernetes offerings from various cloud providers to understanding the physical magic behind the scenes.\nFull disclaimer: This project is largely based-off Anthony Simon\u0026rsquo;s project for a similar build. I found that his blog post lacked a lot of details though so I want to capture those missing parts here and go into more detail about my setup. You can find his great post here!\nYou can find the Ansible scripts and Helm charts that I manage for this project on my Github:\nHelm Charts Ansible Scripts Goals \u0026amp; Requirements Before diving into the build list, architecture, and design for my build I want to review what, for me, were the goals and requirements for setting up this project.\nPortability First, portability. I am in a season of life that is nomadic. I am in different apartments for a year or two at a time. I want a build that I can easily unplug, bring somewhere else, and plug in without needing any extra steps for setup.\nIsolation \u0026amp; Security Second, and closely related, is isolation. I want the network that my cluster runs on to be on a subnet isolated from the LAN network to which it connects. I want all IPs to be in their own non-overlapping address space. I also don\u0026rsquo;t want my service publicly available or available to anyone connected to the LAN of my home network. They should only be accessible via a VPN connection to the cluster network or wireless/wired connection to the cluster LAN.\nPersistent Storage \u0026amp; Back-ups Third, I wanted my cluster to support some implementation PVs and PVCs for data persistence. I wanted this to be affordable and to be reliable. This ruled out buying SSD storage for each node and using a distributed file store like Rook/Ceph or Longhorn. It also ruled out using hostPath storage on SD cards. (Spoiler) My final result uses a single Terabyte SSD that is running as an SMB share which can be mounted via the SMB csi.\nHTTPs My fourth requirement is that all of my services should be available over an HTTPs connection. Sure, the VPN is encrypted, however, I want TLS termination at the cluster and not only the VPN. Further, I don\u0026rsquo;t want browsers complaining that the site I am visiting is not secure. That is a bother for me and a red flag for any friends or family who connect to my services.\nDNS Lastly, I want my services accessible via DNS records when a user is connected via VPN. I want the DNS server to sit on the LAN network and become the primary DNS server for users when they connect to the network. This keeps my A records off of public DNS servers.\nArchitecture The following diagram lays out my planned network topology for this build. I am a big fan of Brandon Sanderson\u0026rsquo;s Stormlight Archives and so I have named my nodes after the Radiant orders.\nI aim to have a 10.100.0.0/24 CIDR allocated to my cluster network. I will have a dedicated router that manages this subnet. The router and nodes will all share a wired ethernet connection through an L2 Gigabit PoE network switch. Within that cluster network, I will configure the DHCP server to assign IPs to the network devices from a subset of the network IPs available. This will allow me, later on, to use another non-overlapping IP range within the cluster CIDR for MetalLB. Kubernetes does not provide a default implementation for allocating IPs to LoadBalancer services. MetalLB is one solution to this problem that I will explore in more depth later on. From the perspective of my cluster network, the home network will be the WAN. All internet-bound traffic will traverse the cluster router gateway and then my home router.\nAnother detail of this design is the SMB share. I have a 1TB SSD physically mounted to one of my RPi nodes. This RPi exposes the 1TB drives as an SMB share that can be mounted by other devices on the network. There is a Kubernetes SMB Container Storage Interface (CSI) driver that supports mounting SMB shares via PVCs. This is how I will implement my poor man\u0026rsquo;s persistent storage.\nNote: This is not intended to be a HA cluster. I only have a single master node. The numbers aren\u0026rsquo;t ideal for consensus. In this build, I just want to learn the basics.\nBuild List The following is my build list for the project:\nUCTronics RPi Cluster Case x1 Raspberry Pi 4b 8Gb x1 Rapsberry Pi 4b 2Gb x3 1 TB SSD x1 SATA to USB 3.0 Adapter x 1 TP-Link 5 port Gigabit PoE Network Switch x1 TP-Link Nano Router x1 0.3M Ethernet Cables x4 RPi PoE Hat x4 Note: I do not receive any commission when you purchase via the above the links. These are just what worked for my build and are what I recommend.\nNetworking With a project like this, you need to start small and work up. Realistically, this means breaking up your end goal into small problems that you can manageably troubleshoot and solve as you go. Trying to take on too much with so many variables and unknowns in one swing will be fatal for a project of this kind. I have broken down this section into the incremental steps I took to accomplish my vision for the networking. These steps were goals I wanted to achieve before approaching the problem of setting up Kubernetes.\nTo start, I linked my TP-Link router and each of the Pis to the network switch via ethernet cables. Using the PoE hat on each Pi the PoE network switch was also able to power the Pis without the need for additional cables.\nConfiguring the Cluster Router When setting up the TP-Link Router the goal is to create a subnet. The TP-Link router will be a gateway to my home network LAN and from there traffic will be routed to the internet via my home network route. To do this, I configured the TP-Link Router in WISP mode. In WISP mode, the router adopts the home network as the WAN network and then broadcasts its own LAN network to which wired/wireless devices can connect. This results in two isolated networks.\nWAN Settings In this configuration, your TP-Link cluster router will be assigned an IP on your home network. The gateway for the cluster router will be the IP of your home network router.\nLAN Settings In the TP-Link router\u0026rsquo;s LAN settings, you\u0026rsquo;ll need to configure the LAN. This is where you can specify the subnet that your cluster nodes will run on. I chose to use the 10.100.0.0/24 CIDR for my cluster network. This gives me 254 IPv4 addresses to work with with (256 minus the broadcast ip and network address) which is more than enough for my little cluster.\nDHCP Settings In the TP-Link router DHCP settings you\u0026rsquo;ll want to configure the IP range (within your LAN subnet) that the DHCP server can pull from when assigning IPs to devices joining the cluster network. A DHCP server is a Dynamic Host Configuration Protocol server. When new devices join a network they send out a discovery call to the DHCP server. The DHCP server then returns an offer containing an IP for the devices to use in the network and other configurations such as the DNS server to use.\nLater on, we\u0026rsquo;ll come back here and configure the DNS.\nStatic Node IPs For this build, I did not want to bother with IPs changing for the nodes in my cluster. For this reason, I assigned each node linked to the network a static IP. You can do this in the DHCP configuration settings of the router so that when the nodes get a new DHCP lease they always get the same IP\nAdding a TailScale Subnet Router Out of my four Raspberry Pis, I have committed three to the cluster and one to running a TailScale SubnetRouter and Pi-Hole. The stoneward node is the Pi that I have chosen to use for hosting my TailScale subnet router and Pi-Hole DNS server. TailScale is a VPN service that builds on top of simplifies Wireguard by delegating the management of peer certificates among new peers to the TailScale control plane. Using TailScale you can run a node as a Subnet Router to route traffic from other users of the VPN network to IP space behind the subnet router. I will take advantage of this functionality by converting the stoneward node into a subnet router that handles routes to my cluster network\u0026rsquo;s CIDR range. This means, that when connected to the VPN, I can reach all of my nodes and services without exposing them to the public internet.\nThe install scripts for TailScale can be found in my Ansible repository. After installing TailScale and advertising my cluster subnet range (note: you have to also approve this node advertising that range in the TailScale Admin console) I then validated that my personal dev laptop could ssh into each of the other nodes linked to my subnet via the PoE network switch.\nK3s For this project I chose to run K3s. K3s is a lightweight Kubernetes binary that runs with significantly smaller memory requirements. I wanted to find a lightweight solution that didn\u0026rsquo;t feel like a compromise and so I was satisfied to run k3s as it is fully Kubernetes compliant, included some functionality out of the box like coredns, and could use an etcd data store.\nInstallation For my K3s installation, I chose to override some of the default tools in favour of tools with which I had more experience. In particular, I replaced the default Flannel CNI with Calico, the Traefik Ingress controller with Nginx Ingress, and ServiceLB with MetalLB. To see all of the installation scripts check out my ansible automation linked at the start of this article. The configurations for my custom tools were installed via Helm and all of the configurations can be found in the Helm Chart repo also linked alongside my Ansible repo.\nOne thing to note about my installation of K3s. K3s supports two types of nodes: k3s-servers and k3s-agents. The k3s-server nodes are responsible for the control plane and datastore components. The k3s-agents do not have any of those responsibilities and just run the kubelet, CRI, and CNI. I chose my 8Gb Raspberry Pi as the single k3s-server node for this cluster. The reasoning for this was two-fold, First I wanted to use etcd since I haven\u0026rsquo;t before. Second, I only had a single SSD for this project and did not want any datastore components running on nodes that only had an unreliable SD card.\nConclusion At this point in time, I had the cluster networking set up and K3s installed on each node. Two of the Raspberry Pis, lightweaver and windrunner, were configured as k3s-agent nodes and Bondsmith, my 8Gb Pi was running as the single k3s-server. With that, I will bring this post to a close. In part 2, I will review my configuration of Nginx Ingress, MetalLB and, and Cert-Manager for managing incoming traffic to my cluster services. Part 2 will also cover how I configured my 1 TB SSD drive as an SMB share to dynamically provision persistent volumes for my workloads.\n","permalink":"http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-1/","summary":"Big Idea From the start of my career, I have been fascinated by Kubernetes. I love distributed systems and the rich history of how we have arrived where we are today with distributed computing. We live in the ongoing evolution of our communal vision for cloud computing. In the early years, the vision was to present a homogenous Unix-like interface for managing an underlying collection of servers such as BOINC. Now, we live in the world of many small virtualized unix environments distributed across servers and sharing compute.","title":"Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis"},{"content":"Big Idea The goal of this post is to capture the steps required to get started with Hugo and GitHub Pages. Hugo is a Go-based static site generation tool. GitHub Pages is a feature of GitHub that allows anyone with a GitHub account to host a static site.\nPart 1: Setting up GitHub Pages In order to serve your site, you will need somewhere to host it. GitHub offers a free service called GitHub Pages that we will use for this purpose. GitHub Pages offers the free hosting of static website content. This means we will be able to build our Hugo site into a static site and then serve that via GitHub Pages.\nCreate GitHub Pages Repo To get started with GitHub Pages for your blog, you will first need a repo in which you\u0026rsquo;ll store your website content. GitHub Pages offers websites for individual projects or for your user. For this tutorial, we will use the user GitHub Pages. The first step to creating this Pages site is creating a repo in your GitHub account that follows the naming scheme: \u0026lt;username\u0026gt;.github.io (substitute \u0026lt;username\u0026gt; with your GitHub username). Select Initialize this repository with a README and then create the repo.\nNote: Your repository must be a public repo for Pages to work.\nAfter creating your repository, navigate to the repository\u0026rsquo;s main page and click on Settings: From the Settings page navigate to the Pages under Code and Automation on the side menu: From here you will want to change your Build and deployment configuration to GitHub Actions. This will be required later when we want to specify a GitHub Actions workflow to build our static site content with Hugo.\nFor now, leave the Custom domain configuration alone. We will return to this at a later step.\nPart 2: Setting Up Hugo The following section covers getting your static site running on your local machine. This will allow you to modify your themes and posts from your editor of choice and see updates via the local Hugo dev server.\nInstall Hugo to your local machine To get started with Hugo on your machine you\u0026rsquo;ll need to first install Hugo. On macOS, you can do this via Brew:\nbrew install hugo Create a New Hugo Site Locally To start a new Hugo project run:\nhugo new site \u0026lt;github-username\u0026gt;.github.io --format yaml Note: Using --format yaml is optional. The default config format for Hugo is toml.\nThis will create a new directory named \u0026lt;github-username\u0026gt;.github.io that is pre-populated with the Hugo starter boilerplate. You don\u0026rsquo;t have to use the site name \u0026lt;github-username\u0026gt;.github.io. You can change it to whatever you would like. However, in the case of GitHub Pages, you created your Pages site in a repo named \u0026lt;github-username\u0026gt;.github.io. To keep naming consistent use the name of the existing repo.\nConnect your local Hugo Project to the Git Repo Now that you have created a new Hugo site you will want to connect it to the GitHub repository you created earlier. To do this we will first initialize your new local project as a git project. This can be done by entering your project directory and then running git init\ncd \u0026lt;github-username\u0026gt;.github.io/ git init This enables version control for your project. Let\u0026rsquo;s create an initial commit of the Hugo site and push to GitHub:\ngit add . git commit -m \u0026#34;Initial commit\u0026#34; git branch -M main git remote add origin \u0026lt;git clone url\u0026gt; git push -u origin main --force ## Use force in this case to override the README in github with new history Note: You can get the git clone url by navigating to your repository, selecting Clone and grabbing the https or ssh clone link. Select the ssh link if you have ssh set up for GitHub. This will keep you from having to enter your credentials on each push.\nAdd a Hugo Theme to your site By default, Hugo does not include a theme for your site. This is where you get to pick how you want your site to look once built and deployed. You can find a complete list of themes on the Hugo themes page. For this example, I will use the theme PaperMod as that is the theme of this blog.\nOnce you have selected a theme, you will want to clone that theme into the /themes directory contained within the Hugo project created above. There are two common ways that others online will recommend doing this. One way requires downloading the theme\u0026rsquo;s repo as a zip file from GitHub, extracting the contents and moving them into the /themes directory. This method does not maintain the git history of the selected theme. It means that as the upstream theme repo changes you will not be able to pull those changes via git. The second method involves cloning the theme repo into /themes and declaring it as a git submodule within the enveloping git repo you created earlier. I don\u0026rsquo;t intend on maintaining my theme heavily so I will not bother with the latter approach. I also find downloading and unzipping tedious. I recommend cloning your theme into /themes and then dropping the .git management from the clone. This is done as follows:\n## Clone PaperMod theme to /themes/PaperMod. Only grab depth 1 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod ## Remove the git history from the cloned PaperMod repo rm -rf /themes/PaperMod/.git Now that you have a theme, you need to tell Hugo to use it. To do so, edit your hugo.yaml and add\ntheme: - PaperMod ## If you chose a different theme put the name of the theme here. This is the folder under /themes that contains the theme Run Hugo Locally You can now run your site locally by running hugo server from the terminal within your project. This brings up your site at http://localhost:1313/.\nYou can change the title on your site by editing the hugo.yaml file. All PaperMod features and customizations can be found documented here: PaperMod Features\nDeploy to GitHub Pages Next, we will deploy to GitHub Pages. Remember that earlier we changed the Build and deployment setting to GitHub Actions. This means that we need to specify a GitHub Actions workflow for GitHub Runners to execute when you push your repo. GitHub Actions are a series of jobs that will be performed on your code base when you push to GitHub. In our case we will use GitHub Actions to build our Hugo site and deploy to Pages. To do this we must create the following workflow file in our repo: \u0026lt;github-username\u0026gt;.github.io/.github/workflows/hugo.yaml\n# Sample workflow for building and deploying a Hugo site to GitHub Pages name: Deploy Hugo site to Pages on: # Runs on pushes targeting the default branch push: branches: - main # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read Pages: write id-token: write # Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued. # However, do NOT cancel in-progress runs as we want to allow these production deployments to complete. concurrency: group: \u0026#34;Pages\u0026#34; cancel-in-progress: false # Default to bash defaults: run: shell: bash jobs: # Build job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.128.0 steps: - name: Install Hugo CLI run: | wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.deb \\ \u0026amp;\u0026amp; sudo dpkg -i ${{ runner.temp }}/hugo.deb - name: Install Dart Sass run: sudo snap install dart-sass - name: Checkout uses: actions/checkout@v4 with: submodules: recursive fetch-depth: 0 - name: Setup Pages id: Pages uses: actions/configure-Pages@v5 - name: Install Node.js dependencies run: \u0026#34;[[ -f package-lock.json || -f npm-shrinkwrap.json ]] \u0026amp;\u0026amp; npm ci || true\u0026#34; - name: Build with Hugo env: HUGO_CACHEDIR: ${{ runner.temp }}/hugo_cache HUGO_ENVIRONMENT: production TZ: America/Los_Angeles run: | hugo \\ --gc \\ --minify \\ --baseURL \u0026#34;${{ steps.Pages.outputs.base_url }}/\u0026#34; - name: Upload artifact uses: actions/upload-Pages-artifact@v3 with: path: ./public # Deployment job deploy: environment: name: github-Pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-Pages@v4 After adding this run:\ngit add . git commit -m \u0026#34;Add theme and release workflow\u0026#34; git push This will push your changes to GitHub. From the GitHub repo page for your project you can open the Actions tab to see your defined workflow running. Once this has completed you can navigate to \u0026lt;github-username\u0026gt;.github.io to view your site! This could take ~10 mins to become visible so don\u0026rsquo;t worry if you don\u0026rsquo;t see it right away.\nPart 3: Adding a Custom Domain To add a custom domain you will need to follow a few steps. First, if you do not have one already, you will need to purchase a domain from a registrar such as Cloudflare, GoDaddy, or NameCheap. Doing this is beyond the scope of this blog and there are multitudes of online guides that will explain this in more detail than I can here.\nSpecify Domain in GitHub Pages After acquiring a domain, return to the Pages tab under Settings \u0026gt; Build and Automation \u0026gt; Pages in GitHub. Here you will see the Custom domain option. Add your domain to the custom domain settings. This can be the Apex domain such as fallow.app if you want your blog to be the root page of your domain. If you\u0026rsquo;d prefer to host your blog on a subdomain such as blog.fallow.app then enter that as your custom domain.. Replace fallow.app with your domain.\nConfigure CNAME Record in your DNS Provider. The registrar from whom you purchased your domain will have DNS settings available for your domain. There are two main DNS record types A records and CNAME records. An A record points to an IP. CNAME records are aliases to other domains. In our case we will create a CNAME record to our GitHub Pages domain. If you are doing this for the subdomain blog.\u0026lt;your domain\u0026gt;, then add blog as your CNAME record and \u0026lt;github-username\u0026gt;.github.io as your target. If you are doing this for the Apex domain (i.e. no subdomain), then use @ (this represents the apex domain) instead of blog.\nNote: DNS records take a while to propagate. You will no longer be able to reach your blog at \u0026lt;github-username\u0026gt;.github.io and it may take 24hrs for your site to become available. In my experience, it has never been that long and has taken at most 30 mins.\n","permalink":"http://localhost:1313/posts/01-getting-started-w-hugo/","summary":"Big Idea The goal of this post is to capture the steps required to get started with Hugo and GitHub Pages. Hugo is a Go-based static site generation tool. GitHub Pages is a feature of GitHub that allows anyone with a GitHub account to host a static site.\nPart 1: Setting up GitHub Pages In order to serve your site, you will need somewhere to host it. GitHub offers a free service called GitHub Pages that we will use for this purpose.","title":"Getting Started with Hugo \u0026 GitHub Pages"}]