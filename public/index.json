[{"content":"Big Idea In part 1 of this post, I covered the basics of getting started building my Kubernetes cluster on Raspberry Pis. In particular, I laid out my goals and requirements, the build list, the network topology and setup, and the installation of K3s on each of the nodes. I recommend going back and checking out that post first if you haven\u0026rsquo;t already (Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis).\nA summary from that post is that I created the subnet 10.100.0.0/24 on my home network for the cluster network. Using DHCP I reserved a small address space of IPs for my nodes and statically assigned the node IPs from within that range. Of my four RPis, three wil be part of the cluster. The fourth node will not run as part of the cluster but will instead run a TailScale subnet router and Pi-Hole for DNS resolution on the cluster network.\nIn this post, I will review how I configured the Nginx Ingress controller and Cert-Manager for managing HTTPs traffic to my cluster. I will also cover my persistent storage solution that implements PV\u0026rsquo;s in Kubernetes using an SMB share. Finally, I will briefly show my backup strategy that leverages rclone and Backblaze B2 storage.\nAs usual, all of my configurations for deploying charts can be found on my GitHub: https://github.com/atmask/helm-charts/tree/main\nLoad-Balancing, Ingress, and SSL/TLS Management Now that, I had my cluster up and running with a CNI installed (I\u0026rsquo;ll do more posts about Calico CNI in the future) I wanted to get the networking setup to access services on my cluster. To achieve this, I added three different installations to my cluster: MetalLB, Nginx Ingress, and Cert-Manager.\nKubernetes has a resource type called Services. Services function as load balancers by providing a single IP for balancing traffic among a backing set of ephemeral pods running a workload. Kubernetes services resources have a few kinds, namely, ClusterIP, NodePort, and LoadBalancer. There are default implementations for the first two service types in Kubernetes but none for LoadBalancer-type services. Most major cloud providers you use will have their own implementation with their Kubernetes offerings that will configure a Load Balancer with a public IP from their platform to manage the incoming traffic to your service. MetalLB is a solution for those running their own Kubernetes clusters to support services of type LoadBalancer.\nNginx-Ingress is an Nginx-based solution for managing network traffic entering the cluster. To use the nginx ingress controller, you expose it behind a LoadBalancer. All incoming traffic can then be routed based on various routing rules such as the path to other services in the cluster. This has the advantage of having single point for managing TLS termination and in cloud environments can save you the cost additional LBs would incur if you exposed each service via an LB.\nFinally, Cert-Manager. Cert-Manager is an X.509 certificate management service for Kubernetes. It integrates nicely with Let\u0026rsquo;s Encrypt for automatically generating and rotating SSL/TLS certs on domains that you own. It also (with some configuration) integrates with Nginx Ingress for automatically provisioning and managing certificates for new domains and subdomains.\nMetalLB Concepts IPPools MetalLB is not able to give you IP addresses out of thin air. This means that you will need to tell it which IP addresses it can alloacte to LoadBalancer services by defining IPPools for your MetalLB installation to use. This IPPool should not overlap with the IP range that the DHCP is configured to assign IPs from. This is where it may be helpful to share the network topolgy again: External Announcements and Layer 2 Routing MetalLB can assign an IP to your LoadBalancer Kubernetes service, however, it also need to make the network outside of your cluster pod-traffic aware of these IPs and to make them routable. MetalLB has two different modes for achieving this goal: BGP and Layer 2. I will focus on Layer 2 mode as that is what I am running and familiar with.\nMetalLB in Layer 2 mode works by taking an available IP from the IPPool that you previous allocated and assigning that IP to a node in your cluster. From an outside perspective, it look as if the node has multiple IPs on the network. This is called Layer 2 mode because of how it makes use of ARP (address resolution protocol). ARP is a protocol that takes place in layer 2 of the OSI networking model. In short, ARP works by the source machine sending out a broadcast message for the destination IP of the packet it is trying to route. If a machine has that IP leased then it responds to the original ARP request by returning its mac address. The mac adress is then used in layer 2 networking to send the packet on to the node in the cluster.\nOnce the packet is routed to the a node in the cluster then kube-proxy takes over and routes the packet to one of the services. kubey-proxy is an agent that manages iptables in the cluster to support the routing of packaets from the virtual ips of services to the ips of pods assigned via the CNI. This may be the subject of another post in the future but for now I\u0026rsquo;d refer you to this article: Kube-Proxy: Waht is it and How it Works\nDeploying MetalLB Now, to the fun part! MetalLB can be deployed via Helm charts to you cluster. The Helm chart can be found on Artifact Hub. The first install of the MetalLB chaty aditionally installs custom CRDs to the cluster. CRDs are \u0026ldquo;Custom Resource Definitions\u0026rdquo; and allow for the creation custom resources like pods or deployments but more relevant to a speicifc application. In the case of MetalLB we care about the IPAddressPool and L2Advertisement CRDs. After the initial install of the chart we will want to deploy and IPAddressPool resource to to tell MetalLB what IP range we have set aside in our subnet for LoadBalancer IPs. We will also deploy an L2Advertisement resource that tells Metallb to advertise ips in that pool via Layer 2 networking.\nI prefer to use Helm\u0026rsquo;s subcharting functionality to keep all of my chart configurations and version in VCS over time but, in general, the installation process would look like this:\nhelm install -n metallb metallb oci://registry-1.docker.io/bitnamicharts/metallb --create-namespace Then create the IPAddressPool in ippool.yaml\n# The address-pools lists the IP addresses that MetalLB is # allowed to allocate. You can have as many # address pools as you want. apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: # A name for the address pool. Services can request allocation # from a specific address pool using this name. name: lb-ip-pool namespace: metallb-system spec: # A list of IP address ranges over which MetalLB has # authority. You can list multiple ranges in a single pool, they # will all share the same settings. Each range can be either a # CIDR prefix, or an explicit start-end range of IPs. addresses: - 10.100.0.50-10.100.0.75 and the L2Advertisement in advertisement-l2.yaml\napiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: ip-pool-advertisement namespace: metallb-system spec: ipAddressPools: - lb-ip-pool then apply both files to the cluster:\nkubectl apply -n metallb -f ippool.yaml kubectl apply -n metallb -f advertisement-l2.yaml Note: This is the quick and dirty way to do this. I reccommend checking out my repo linked at the start of this article to see how subchartting can be used for maintaining the configuration of third-party charts.\nValidate with LoadBalancer If you don\u0026rsquo;t have any applications deployed and want to validate that the metallb installation is wokring you can apply the following LoadBalancer Service to you cluster and verify that an IP from your IPPool is attached to the service and an external-ip\n### lb-svc.yaml apiVersion: v1 kind: Service metadata: name: my-service namespace: default spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 type: LoadBalancer and apply with kubectl apply -f lb-svc.yaml. Then use kubectl get svc to verify that an external-ip has been assigned to the service named \u0026ldquo;my-service\u0026rdquo;.\nDeploying Nginx-Ingress Now that we can provision IPs for an implementation of LoadBalancer-type services we can move on to our installation of Nginx Ingress. Yes, technically, NodePort could have been used instead of setting up MetalLB but this is about leraning new things! The Nginx Ingresss controller will act as a single point of entry for all traffic to workloads running in my cluster. Nginx Ingress supports host and path-based routing and I will make use of this when setting DNS records for my apps later on. A large benefit of using Nginx Ingress as a single point of entry for all incoming traffic is that I can integrat Nginx Ingress with Cert-Manager so that it is also the single point for managing all TLS termination of incoming traffic. This reduces the management overhead for me.\nAs with MetalLB, my configuration of the Nginx Ingress controller chart can be found in my charts repo linked at the top of this post. There is a lot less configuration to do for this deployment. I will provide a quick and dirty installation here as well though. All that you need to configure via the values file is the service resource that will act as the load balancer to the ingress controller.\n### nginx-ingress-values.yaml controller: service: enabled: true type: \u0026#34;LoadBalancer\u0026#34; annotations: metallb.universe.tf/address-pool: lb-ip-pool # Add the annotation so that metallb will use the previously configured ippool and then install:\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install -n nginx-ingress nginx-ingress ingress-nginx/ingress-nginx -f nginx-ingress-values.yaml --create-namespace Hura-ah! The Nginx Ingress controller should now be installed. It will watch of the creation ingress resources in the cluster. ingress resources are typically deployed with your application and configure how the Nginx Ingress Controller should route traffic to that application.\nConfiguring Cert-Manager As of right now, with MetalLB and the Nginx Ingress controller set up, I would be ready to access my applications. The problem is, any web browser will give me a page saying that the page I am trying to access is insecure and I would have to click through to access my applications. This is an annoyance and so I have setup Cert-Manager to solve this problem.\nCert-Manager is a tool that can be integrated Nginx Ingress to automatically provision and rotate SSL/TLS certs for your domains. The SSL/TLS certs are used to encrypt the HTTP traffic between your servers and clients. This is what give us HTTPs.\nAn important thing to know before setting up Cert-Manager is that SSL/TLS certificates can only be issued for domains that you own. When Cert-Manager goes to provision or rotate a cert for your service it must pass one of two tests known as ACME challenges (DNS-01 or HTTP-01) to verify that you are the owner of the domain for which you are provisioning a certificate.\nInstalling Cert-Manager Cert-Manager, like MetalLB, has a set of CRDs that are needed for configuring the installation. This means that I had to to do the initial Helm install to get the CRDs and then a subsuquent Helm upgrade to add configurations. There is one additional CRD needed when setting up Cert-Manager. The required CRD, for my use case and setup at least, is the ClusterIssuer. The ClusterIssuer configures which Certificate Authority (CA) Cert-Manager will use to generate the SSL/TLS certificates. In my installation I have used Let\u0026rsquo;s Encrypt as my CA. Let\u0026rsquo;s Encrypt provides free certificates and has a generous quota on their production service.\nThe initial Cert-Manager installation can be added as follows:\nhelm repo add jetstack https://charts.jetstack.io --force-update helm install cert-manager -n cert-manager --version v1.15.1 jetstack/cert-manager --set installCRDs=true --create-namespace After the initial installation, I deployed the ClusterIssuer. As I mentioned earlier, certificates can only be issued for domains that you own. This means that the CA you choose will attempt to execute a DNS-01 or HTTP-01 challenge successfully before provisioning your cert. DNS-01 is more secure so I cover that here. With DNS-01 you must configure your ClusterIssuer with an API key for the registrar where you manage your domain. In my case, this is Cloudflare. The DNS-01 challenge works by the CA requesting that you configure a TXT record on your domain with specific values that the CA provides under the sudomain _acme-challenge.\u0026lt;YOUR_DOMAIN\u0026gt;. If the CA validates that this has been done, then your ownership of the domain is verified and the certificate is issued. Using my provided API key, Cert-Manager will do this all on my behalf.\n## cluster-issuer.yaml apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt spec: acme: # You must replace this email address with your own. # Let\u0026#39;s Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: \u0026lt;your email\u0026gt; server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource that will be used to store the account\u0026#39;s private key. name: \u0026lt;arbitrary secret name where your acme account key will be stored once generated\u0026gt; # Add a single challenge solver solvers: - dns01: cloudflare: apiTokenSecretRef: name: registrar-api-key key: api-token ## secret.yaml apiVersion: v1 kind: Secret metadata: name: registrar-api-key type: Opaque stringData: api-token: {{ .Values.cloudflareApiToken }} Note: If you are setting this up for the first time it is reccommended to use the let\u0026rsquo;s encrypt staging servers so as to not waste your quota. This is done by replacing acme-v02 with acme-staging-v02 in the server configuration.\nWith Cert-Manager and a ClusterIssuer installed, I was now able to create new ingress resources that containing routing rules and integrate with Cert-Manager via annotations for the automatic provisioning of SSL/TLS certs. Using the API token for Cloudflare, Cert-Manager completes the DNS-01 ACME challenge on my behalf.\nLocal DNS Management with Pi-Hole On more small detail about my cluster pertains to DNS A records. I did not want all of the DNS records for my services available on the public internet. Although the services running on my local network would be unreachable to anyone on the public internet, storing the DNS records with the private IPs for my hosted services in Cloudflare or any other public DNS provider would have meant that anyone could discover the private IPs at which I host services. For me, this was not ideal. This was the motivation for setting up Pi-Hole as a DNS server on my home network. Pi-Hole will act like an Azure Private DNS Zone or Private Hosted Zones in AWS Route53.\nThe first step to achieving this goal was installing Pi-Hole on one of my Pis. I chose to do the installation on stoneward (refer to the network diagram for the naming of my nodes). This is the same Pi that runs TailScale and does not serve any role in the K3s cluster. After completing the Pi-Hole installation you can navigate to Local DNS \u0026gt; DNS Records in the admin web ui and configure DNS records for reaching the ngin-ingress controller and other commons IPs such as the cluster node IPs.\nThe second and final setup\nPersistent Storage Creating the SMB share Setting up the SMB CSI Driver Creating a PVC Backups with rsync and Backblaze ","permalink":"http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-2/","summary":"Big Idea In part 1 of this post, I covered the basics of getting started building my Kubernetes cluster on Raspberry Pis. In particular, I laid out my goals and requirements, the build list, the network topology and setup, and the installation of K3s on each of the nodes. I recommend going back and checking out that post first if you haven\u0026rsquo;t already (Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis).","title":"Part 2: Building a Bare-metal Kubernetes Cluster on Raspberry Pis"},{"content":"Big Idea From the start of my career, I have been fascinated by Kubernetes. I love distributed systems and the rich history of how we have arrived where we are today with distributed computing. We live in the ongoing evolution of our communal vision for cloud computing. In the early years, the vision was to present a homogenous Unix-like interface for managing an underlying collection of servers such as BOINC. Now, we live in the world of many small virtualized unix environments distributed across servers and sharing compute.\nOne of the main drivers of our current state has been the advent of containerization and container orchestration. The goal of this blog is to go over the considerations and design of my bare-metal Raspberry Pi Kubernetes cluster. This project was my adventure in going beyond being a user of Kubernetes offerings from various cloud providers to understanding the physical magic behind the scenes.\nFull disclaimer: This project is largely based-off Anthony Simon\u0026rsquo;s project for a similar build. I found that his blog post lacked a lot of details though so I want to capture those missing parts here and go into more detail about my setup. You can find his great post here!\nYou can find the Ansible scripts and Helm charts that I manage for this project on my Github:\nHelm Charts Ansible Scripts Goals \u0026amp; Requirements Before diving into the build list, architecture, and design for my build I want to review what, for me, were the goals and requirements for setting up this project.\nPortability First, portability. I am in a season of life that is nomadic. I am in different apartments for a year or two at a time. I want a build that I can easily unplug, bring somewhere else, and plug in without needing any extra steps for setup.\nIsolation \u0026amp; Security Second, and closely related, is isolation. I want the network that my cluster runs on to be on a subnet isolated from the LAN network to which it connects. I want all IPs to be in their own non-overlapping address space. I also don\u0026rsquo;t want my services available publicly or to anyone connected to the LAN of my home network. They should only be accessible via a VPN connection to the cluster network or wireless/wired connection to the cluster LAN.\nPersistent Storage \u0026amp; Back-ups Third, I wanted my cluster to support some implementation PVs and PVCs for data persistence. I wanted this to be affordable and to be reliable. This ruled out buying SSD storage for each node and using a distributed file store like Rook/Ceph or Longhorn. It also ruled out using hostPath storage on SD cards. (Spoiler) My final result uses a single Terabyte SSD that is running as an SMB share which can be mounted via the SMB csi.\nHTTPs My fourth requirement is that all of my services should be available over an HTTPs connection. Sure, the VPN is encrypted, however, I want TLS termination at the cluster and not only the VPN. Further, I don\u0026rsquo;t want browsers complaining that the site I am visiting is not secure. That is a bother for me and a red flag for any friends or family who connect to my services.\nDNS Lastly, I want my services accessible via DNS records when a user is connected via VPN. I want the DNS server to sit on the cluster LAN network and become the primary DNS server for users when they connect to the network. This keeps my A records off of public DNS servers.\nArchitecture The following diagram lays out my planned network topology for this build. I am a big fan of Brandon Sanderson\u0026rsquo;s Stormlight Archives and so I have named my nodes after the Radiant orders.\nI aim to have a 10.100.0.0/24 CIDR allocated to my cluster network. I will have a dedicated router that manages this subnet. The router and nodes will all share a wired ethernet connection through an L2 Gigabit PoE network switch. Within that cluster network, I will configure the DHCP server to assign IPs to the network devices from a subset of the network IPs available. This will allow me, later on, to use another non-overlapping IP range within the cluster CIDR for MetalLB. Kubernetes does not provide a default implementation for allocating IPs to LoadBalancer services. MetalLB is one solution to this problem that I will explore in more depth later on. From the perspective of my cluster network, the home network will be the WAN. All internet-bound traffic will traverse the cluster router gateway and then my home router.\nAnother detail of this design is the SMB share. I have a 1TB SSD physically mounted to one of my RPi nodes. This RPi exposes the 1TB drives as an SMB share that can be mounted by other devices on the network. There is a Kubernetes SMB Container Storage Interface (CSI) driver that supports mounting SMB shares via PVCs. This is how I will implement my poor man\u0026rsquo;s persistent storage.\nNote: This is not intended to be a HA cluster. I only have a single master node. The numbers aren\u0026rsquo;t ideal for consensus. In this build, I just want to learn the basics.\nBuild List The following is my build list for the project:\nUCTronics RPi Cluster Case x1 Raspberry Pi 4b 8Gb x1 Rapsberry Pi 4b 2Gb x3 1 TB SSD x1 SATA to USB 3.0 Adapter x 1 TP-Link 5 port Gigabit PoE Network Switch x1 TP-Link Nano Router x1 0.3M Ethernet Cables x4 RPi PoE Hat x4 Note: I do not receive any commission when you purchase via the above the links. These are just what worked for my build and are what I recommend.\nNetworking With a project like this, you need to start small and work up. Realistically, this means breaking up your end goal into small problems that you can manageably troubleshoot and solve as you go. Trying to take on too much with so many variables and unknowns in one swing will be fatal for a project of this kind. I have broken down this section into the incremental steps I took to accomplish my vision for the networking. These steps were goals I wanted to achieve before approaching the problem of setting up Kubernetes.\nTo start, I linked my TP-Link router and each of the Pis to the network switch via ethernet cables. Using the PoE hat on each Pi the PoE network switch was also able to power the Pis without the need for additional cables.\nConfiguring the Cluster Router When setting up the TP-Link Router the goal is to create a subnet. The TP-Link router will be a gateway to my home network LAN and from there traffic will be routed to the internet via my home network route. To do this, I configured the TP-Link Router in WISP mode. In WISP mode, the router adopts the home network as the WAN network and then broadcasts its own LAN network to which wired/wireless devices can connect. This results in two isolated networks.\nWAN Settings In this configuration, your TP-Link cluster router will be assigned an IP on your home network. The gateway for the cluster router will be the IP of your home network router.\nLAN Settings In the TP-Link router\u0026rsquo;s LAN settings, you\u0026rsquo;ll need to configure the LAN. This is where you can specify the subnet that your cluster nodes will run on. I chose to use the 10.100.0.0/24 CIDR for my cluster network. This gives me 254 IPv4 addresses to work with with (256 minus the broadcast ip and network address) which is more than enough for my little cluster.\nDHCP Settings In the TP-Link router DHCP settings you\u0026rsquo;ll want to configure the IP range (within your LAN subnet) that the DHCP server can pull from when assigning IPs to devices joining the cluster network. A DHCP server is a Dynamic Host Configuration Protocol server. When new devices join a network they send out a discovery call to the DHCP server. The DHCP server then returns an offer containing an IP for the devices to use in the network and other configurations such as the DNS server to use.\nLater on, we\u0026rsquo;ll come back here and configure the DNS.\nStatic Node IPs For this build, I did not want to bother with IPs changing for the nodes in my cluster. For this reason, I assigned each node linked to the network a static IP. You can do this in the DHCP configuration settings of the router so that when the nodes get a new DHCP lease they always get the same IP\nAdding a TailScale Subnet Router Out of my four Raspberry Pis, I have committed three to the cluster and one to running a TailScale subnet router and Pi-Hole. The stoneward node is the Pi that I have chosen to use for hosting my TailScale subnet router and Pi-Hole DNS server. TailScale is a VPN service that builds on top of simplifies Wireguard by delegating the management of peer certificates among new peers to the TailScale control plane. Using TailScale you can run a node as a Subnet Router to route traffic from other users of the VPN network to IP space behind the subnet router. I will take advantage of this functionality by converting the stoneward node into a subnet router that handles routes to my cluster network\u0026rsquo;s CIDR range. This means, that when connected to the VPN, I can reach all of my nodes and services without exposing them to the public internet.\nThe install scripts for TailScale can be found in my Ansible repository. After installing TailScale and advertising my cluster subnet range (note: you have to also approve this node advertising that range in the TailScale Admin console) I then validated that my personal dev laptop could ssh into each of the other nodes linked to my subnet via the PoE network switch.\nK3s For this project I chose to run K3s. K3s is a lightweight Kubernetes binary that runs with significantly smaller memory requirements. I wanted to find a lightweight solution that didn\u0026rsquo;t feel like a compromise and so I was satisfied to run k3s as it is fully Kubernetes compliant, included some functionality out of the box like coredns, and could use an etcd data store.\nInstallation For my K3s installation, I chose to override some of the default tools in favour of tools with which I had more experience. In particular, I replaced the default Flannel CNI with Calico, the Traefik Ingress controller with Nginx Ingress, and ServiceLB with MetalLB. To see all of the installation scripts check out my ansible automation linked at the start of this article. The configurations for my custom tools were installed via Helm and all of the configurations can be found in the Helm Chart repo also linked alongside my Ansible repo.\nOne thing to note about my installation of K3s. K3s supports two types of nodes: k3s-servers and k3s-agents. The k3s-server nodes are responsible for the control plane and datastore components. The k3s-agents do not have any of those responsibilities and just run the kubelet, CRI, and CNI. I chose my 8Gb Raspberry Pi as the single k3s-server node for this cluster. The reasoning for this was two-fold, First I wanted to use etcd since I haven\u0026rsquo;t before. Second, I only had a single SSD for this project and did not want any datastore components running on nodes that only had an unreliable SD card.\nConclusion At this point in time, I had the cluster networking set up and K3s installed on each node. Two of the Raspberry Pis, lightweaver and windrunner, were configured as k3s-agent nodes and Bondsmith, my 8Gb Pi, was running as the single k3s-server. With that, I will bring this post to a close. In part 2, I will review my configuration of Nginx Ingress, MetalLB and, and Cert-Manager for managing incoming traffic to my cluster services. Part 2 will also cover how I configured my 1 TB SSD drive as an SMB share to dynamically provision persistent volumes for my workloads.\n","permalink":"http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-1/","summary":"Big Idea From the start of my career, I have been fascinated by Kubernetes. I love distributed systems and the rich history of how we have arrived where we are today with distributed computing. We live in the ongoing evolution of our communal vision for cloud computing. In the early years, the vision was to present a homogenous Unix-like interface for managing an underlying collection of servers such as BOINC. Now, we live in the world of many small virtualized unix environments distributed across servers and sharing compute.","title":"Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis"},{"content":"Big Idea The goal of this post is to capture the steps required to get started with Hugo and GitHub Pages. Hugo is a Go-based static site generation tool. GitHub Pages is a feature of GitHub that allows anyone with a GitHub account to host a static site.\nPart 1: Setting up GitHub Pages In order to serve your site, you will need somewhere to host it. GitHub offers a free service called GitHub Pages that we will use for this purpose. GitHub Pages offers the free hosting of static website content. This means we will be able to build our Hugo site into a static site and then serve that via GitHub Pages.\nCreate GitHub Pages Repo To get started with GitHub Pages for your blog, you will first need a repo in which you\u0026rsquo;ll store your website content. GitHub Pages offers websites for individual projects or for your user. For this tutorial, we will use the user GitHub Pages. The first step to creating this Pages site is creating a repo in your GitHub account that follows the naming scheme: \u0026lt;username\u0026gt;.github.io (substitute \u0026lt;username\u0026gt; with your GitHub username). Select Initialize this repository with a README and then create the repo.\nNote: Your repository must be a public repo for Pages to work.\nAfter creating your repository, navigate to the repository\u0026rsquo;s main page and click on Settings: From the Settings page navigate to the Pages under Code and Automation on the side menu: From here you will want to change your Build and deployment configuration to GitHub Actions. This will be required later when we want to specify a GitHub Actions workflow to build our static site content with Hugo.\nFor now, leave the Custom domain configuration alone. We will return to this at a later step.\nPart 2: Setting Up Hugo The following section covers getting your static site running on your local machine. This will allow you to modify your themes and posts from your editor of choice and see updates via the local Hugo dev server.\nInstall Hugo to your local machine To get started with Hugo on your machine you\u0026rsquo;ll need to first install Hugo. On macOS, you can do this via Brew:\nbrew install hugo Create a New Hugo Site Locally To start a new Hugo project run:\nhugo new site \u0026lt;github-username\u0026gt;.github.io --format yaml Note: Using --format yaml is optional. The default config format for Hugo is toml.\nThis will create a new directory named \u0026lt;github-username\u0026gt;.github.io that is pre-populated with the Hugo starter boilerplate. You don\u0026rsquo;t have to use the site name \u0026lt;github-username\u0026gt;.github.io. You can change it to whatever you would like. However, in the case of GitHub Pages, you created your Pages site in a repo named \u0026lt;github-username\u0026gt;.github.io. To keep naming consistent use the name of the existing repo.\nConnect your local Hugo Project to the Git Repo Now that you have created a new Hugo site you will want to connect it to the GitHub repository you created earlier. To do this we will first initialize your new local project as a git project. This can be done by entering your project directory and then running git init\ncd \u0026lt;github-username\u0026gt;.github.io/ git init This enables version control for your project. Let\u0026rsquo;s create an initial commit of the Hugo site and push to GitHub:\ngit add . git commit -m \u0026#34;Initial commit\u0026#34; git branch -M main git remote add origin \u0026lt;git clone url\u0026gt; git push -u origin main --force ## Use force in this case to override the README in github with new history Note: You can get the git clone url by navigating to your repository, selecting Clone and grabbing the https or ssh clone link. Select the ssh link if you have ssh set up for GitHub. This will keep you from having to enter your credentials on each push.\nAdd a Hugo Theme to your site By default, Hugo does not include a theme for your site. This is where you get to pick how you want your site to look once built and deployed. You can find a complete list of themes on the Hugo themes page. For this example, I will use the theme PaperMod as that is the theme of this blog.\nOnce you have selected a theme, you will want to clone that theme into the /themes directory contained within the Hugo project created above. There are two common ways that others online will recommend doing this. One way requires downloading the theme\u0026rsquo;s repo as a zip file from GitHub, extracting the contents and moving them into the /themes directory. This method does not maintain the git history of the selected theme. It means that as the upstream theme repo changes you will not be able to pull those changes via git. The second method involves cloning the theme repo into /themes and declaring it as a git submodule within the enveloping git repo you created earlier. I don\u0026rsquo;t intend on maintaining my theme heavily so I will not bother with the latter approach. I also find downloading and unzipping tedious. I recommend cloning your theme into /themes and then dropping the .git management from the clone. This is done as follows:\n## Clone PaperMod theme to /themes/PaperMod. Only grab depth 1 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod ## Remove the git history from the cloned PaperMod repo rm -rf /themes/PaperMod/.git Now that you have a theme, you need to tell Hugo to use it. To do so, edit your hugo.yaml and add\ntheme: - PaperMod ## If you chose a different theme put the name of the theme here. This is the folder under /themes that contains the theme Run Hugo Locally You can now run your site locally by running hugo server from the terminal within your project. This brings up your site at http://localhost:1313/.\nYou can change the title on your site by editing the hugo.yaml file. All PaperMod features and customizations can be found documented here: PaperMod Features\nDeploy to GitHub Pages Next, we will deploy to GitHub Pages. Remember that earlier we changed the Build and deployment setting to GitHub Actions. This means that we need to specify a GitHub Actions workflow for GitHub Runners to execute when you push your repo. GitHub Actions are a series of jobs that will be performed on your code base when you push to GitHub. In our case we will use GitHub Actions to build our Hugo site and deploy to Pages. To do this we must create the following workflow file in our repo: \u0026lt;github-username\u0026gt;.github.io/.github/workflows/hugo.yaml\n# Sample workflow for building and deploying a Hugo site to GitHub Pages name: Deploy Hugo site to Pages on: # Runs on pushes targeting the default branch push: branches: - main # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read Pages: write id-token: write # Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued. # However, do NOT cancel in-progress runs as we want to allow these production deployments to complete. concurrency: group: \u0026#34;Pages\u0026#34; cancel-in-progress: false # Default to bash defaults: run: shell: bash jobs: # Build job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.128.0 steps: - name: Install Hugo CLI run: | wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.deb \\ \u0026amp;\u0026amp; sudo dpkg -i ${{ runner.temp }}/hugo.deb - name: Install Dart Sass run: sudo snap install dart-sass - name: Checkout uses: actions/checkout@v4 with: submodules: recursive fetch-depth: 0 - name: Setup Pages id: Pages uses: actions/configure-Pages@v5 - name: Install Node.js dependencies run: \u0026#34;[[ -f package-lock.json || -f npm-shrinkwrap.json ]] \u0026amp;\u0026amp; npm ci || true\u0026#34; - name: Build with Hugo env: HUGO_CACHEDIR: ${{ runner.temp }}/hugo_cache HUGO_ENVIRONMENT: production TZ: America/Los_Angeles run: | hugo \\ --gc \\ --minify \\ --baseURL \u0026#34;${{ steps.Pages.outputs.base_url }}/\u0026#34; - name: Upload artifact uses: actions/upload-Pages-artifact@v3 with: path: ./public # Deployment job deploy: environment: name: github-Pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-Pages@v4 After adding this run:\ngit add . git commit -m \u0026#34;Add theme and release workflow\u0026#34; git push This will push your changes to GitHub. From the GitHub repo page for your project you can open the Actions tab to see your defined workflow running. Once this has completed you can navigate to \u0026lt;github-username\u0026gt;.github.io to view your site! This could take ~10 mins to become visible so don\u0026rsquo;t worry if you don\u0026rsquo;t see it right away.\nPart 3: Adding a Custom Domain To add a custom domain you will need to follow a few steps. First, if you do not have one already, you will need to purchase a domain from a registrar such as Cloudflare, GoDaddy, or NameCheap. Doing this is beyond the scope of this blog and there are multitudes of online guides that will explain this in more detail than I can here.\nSpecify Domain in GitHub Pages After acquiring a domain, return to the Pages tab under Settings \u0026gt; Build and Automation \u0026gt; Pages in GitHub. Here you will see the Custom domain option. Add your domain to the custom domain settings. This can be the Apex domain such as fallow.app if you want your blog to be the root page of your domain. If you\u0026rsquo;d prefer to host your blog on a subdomain such as blog.fallow.app then enter that as your custom domain.. Replace fallow.app with your domain.\nConfigure CNAME Record in your DNS Provider. The registrar from whom you purchased your domain will have DNS settings available for your domain. There are two main DNS record types A records and CNAME records. An A record points to an IP. CNAME records are aliases to other domains. In our case we will create a CNAME record to our GitHub Pages domain. If you are doing this for the subdomain blog.\u0026lt;your domain\u0026gt;, then add blog as your CNAME record and \u0026lt;github-username\u0026gt;.github.io as your target. If you are doing this for the Apex domain (i.e. no subdomain), then use @ (this represents the apex domain) instead of blog.\nNote: DNS records take a while to propagate. You will no longer be able to reach your blog at \u0026lt;github-username\u0026gt;.github.io and it may take 24hrs for your site to become available. In my experience, it has never been that long and has taken at most 30 mins.\n","permalink":"http://localhost:1313/posts/01-getting-started-w-hugo/","summary":"Big Idea The goal of this post is to capture the steps required to get started with Hugo and GitHub Pages. Hugo is a Go-based static site generation tool. GitHub Pages is a feature of GitHub that allows anyone with a GitHub account to host a static site.\nPart 1: Setting up GitHub Pages In order to serve your site, you will need somewhere to host it. GitHub offers a free service called GitHub Pages that we will use for this purpose.","title":"Getting Started with Hugo \u0026 GitHub Pages"}]