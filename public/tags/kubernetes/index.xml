<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Kubernetes on Ben Mask</title>
    <link>http://localhost:1313/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on Ben Mask</description>
    <generator>Hugo -- 0.152.2</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 14 Dec 2025 22:43:07 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Immich: Up &amp; Running</title>
      <link>http://localhost:1313/posts/14-immich/</link>
      <pubDate>Sun, 14 Dec 2025 22:43:07 -0500</pubDate>
      <guid>http://localhost:1313/posts/14-immich/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;I recently started hosting &lt;a href=&#34;https://github.com/immich-app&#34;&gt;Immich&lt;/a&gt; on my homelab. If you&amp;rsquo;re not familiar with Immich, it is a self-hosted Google Photos replacement.
If you want to learn more I highly reccomend giving a listen to &lt;a href=&#34;https://selfhosted.show/110?t=1198&#34;&gt;this podcast interview&lt;/a&gt; with the creator of Immich on the Self-Hosted podcast.&lt;/p&gt;
&lt;p&gt;The goal of this post is to cover how I set this up and some of the obstacles I faced in the process.&lt;/p&gt;
&lt;h1 id=&#34;immich-architecture-and-storage&#34;&gt;Immich Architecture and Storage&lt;/h1&gt;
&lt;p&gt;There wasn&amp;rsquo;t much to plan out before deploying Immich. The main pain point came with respect to getting Postgres running up on my cluster prior to deploying Immich which I&amp;rsquo;ll cover more below.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Making my K8s Control Plane HA</title>
      <link>http://localhost:1313/posts/11-ha-k8s/</link>
      <pubDate>Sun, 23 Feb 2025 21:10:27 -0500</pubDate>
      <guid>http://localhost:1313/posts/11-ha-k8s/</guid>
      <description>&lt;h1 id=&#34;big-idea&#34;&gt;Big Idea&lt;/h1&gt;
&lt;p&gt;For a while the K8s cluster in my homelab has been in the precarious situation of only having a single master node. This has bitten me a couple of times when the master node (i.e the &amp;ldquo;server node&amp;rdquo; in k3s) has been snafu&amp;rsquo;ed.&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;./images/meme.png&#34; alt=&#34;meme&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;Now that I have compute available on my proxmox server, I want to take the time to virtualize two additional master nodes. Doing this will require understanding a few things at the outset:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Intro to K8s Autoscaling</title>
      <link>http://localhost:1313/posts/05-intro-to-k8s-autoscaling/</link>
      <pubDate>Fri, 06 Sep 2024 19:57:08 -0400</pubDate>
      <guid>http://localhost:1313/posts/05-intro-to-k8s-autoscaling/</guid>
      <description>&lt;h1 id=&#34;big-idea&#34;&gt;Big Idea&lt;/h1&gt;
&lt;p&gt;Autoscaling is a key technology that builds on the elasticity of distributed cloud computing. To understand autoscaling in the Kubernetes environment, it will be important to understand the basic enabling technologies: containerization and distributed computing. By leveraging hese two technologies, Kubernetes supports horizontal pod autoscaling (hpa) and cluster autoscaling. In this post I will dive into some brief background on Containerization and distributed computing (just enough to be dangerous) and then take a more focused look at the horizontal autoscaling functionality of Kubernetes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 2: Building a Bare-metal Kubernetes Cluster on Raspberry Pis</title>
      <link>http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-2/</link>
      <pubDate>Sun, 21 Jul 2024 20:30:55 -0400</pubDate>
      <guid>http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-2/</guid>
      <description>&lt;h1 id=&#34;big-idea&#34;&gt;Big Idea&lt;/h1&gt;
&lt;p&gt;In part 1 of this post, I covered the basics of getting started building my Kubernetes cluster on Raspberry Pis. In particular, I laid out my goals and requirements, the build list, the network topology and setup, and the installation of K3s on each of the nodes. I recommend going back and checking out that post first if you haven&amp;rsquo;t already (&lt;a href=&#34;https://blog.fallow.app/posts/02-bare-metal-k3s-on-rpi-part-1/&#34;&gt;Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis</title>
      <link>http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-1/</link>
      <pubDate>Wed, 10 Jul 2024 20:29:01 -0400</pubDate>
      <guid>http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-1/</guid>
      <description>&lt;h1 id=&#34;big-idea&#34;&gt;Big Idea&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;images/cluster.png&#34; alt=&#34;cluster&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;From the start of my career, I have been fascinated by Kubernetes. I love distributed systems and the rich history of how we have arrived where we are today with distributed computing. We live in the ongoing evolution of our communal vision for cloud computing. In the early years, the vision was to present a homogenous Unix-like interface for managing an underlying collection of servers such as BOINC. Now, we live in the world of many small virtualized unix environments distributed across servers and sharing compute.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
